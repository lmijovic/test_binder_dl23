{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "\n",
    "\n",
    "def classifier(num_feat):\n",
    "    # Inputs\n",
    "    i = Input(shape = (num_feat,))\n",
    "    \n",
    "    # Hidden layers\n",
    "    x1 = Dense(24, activation = 'relu')(i)      \n",
    "    x2 = Dense(16, activation = 'relu')(x1)     \n",
    "    x3 = Dense(8, activation = 'relu')(x2)      \n",
    "    \n",
    "    # Output layer\n",
    "    o = Dense(1, activation = 'sigmoid')(x3)\n",
    "    \n",
    "    # Build NN classifier\n",
    "    return Model(inputs = i, outputs = o, name = 'classifier')\n",
    "\n",
    "\n",
    "def adversary(num_gmm):\n",
    "    # Inputs\n",
    "    i = Input(shape = (1,))\n",
    "    myy = Input(shape = (1,))\n",
    "    \n",
    "    # Hidden layers\n",
    "    x1 = Dense(200, activation = 'relu')(i)       \n",
    "    x2 = Dense(100, activation = 'relu')(x1)      \n",
    "    x3 = Dense(50, activation = 'relu')(x2)      \n",
    "    \n",
    "    # Gaussian mixture model (GMM):\n",
    "    # The myy distribution guessed by Adversary is represented as a mixture of Gaussians;\n",
    "    # the adversary is tasked with setting the normalization coefficients,\n",
    "    # means, and widths of these Gaussians \n",
    "    coeffs = Dense(num_gmm, activation='softmax')(x3)  # GMM coefficients sum to one\n",
    "    means  = Dense(num_gmm, activation='sigmoid')(x3)  # Means are on [0, 1]\n",
    "    widths = Dense(num_gmm, activation='softplus')(x3)  # Widths are positive\n",
    "    \n",
    "    # Posterior probability distribution function\n",
    "    pdf = layers.PosteriorLayer(num_gmm)([coeffs, means, widths, myy])\n",
    "\n",
    "    return Model(inputs = [i, myy], outputs = pdf, name = 'adversary')\n",
    "\n",
    "\n",
    "# ANN;\n",
    "# lr_ratio = learning rate ratio\n",
    "def combined(clf, adv, lambda_reg, lr_ratio):\n",
    "    # Inputs\n",
    "    clf_input = Input(shape = clf.layers[0].input_shape[0][1])\n",
    "    myy_input = Input(shape = (1,))\n",
    "    \n",
    "    # Classifier ouput\n",
    "    clf_output = clf(clf_input)\n",
    "    \n",
    "    # Gradient reversal\n",
    "    gradient_reversal = layers.GradientReversalLayer(lambda_reg * lr_ratio)(clf_output)\n",
    "    \n",
    "    # Adversary\n",
    "    adv_output = adv([gradient_reversal, myy_input])\n",
    "    \n",
    "    return Model(inputs=[clf_input, myy_input], outputs=[clf_output, adv_output], name='combined')\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Kullback-Leibler loss; maximises posterior p.d.f.\n",
    "    Equivalent to binary-cross-entropy for all y = 1\n",
    "    '''    \n",
    "    return -K.log(y_pred)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#=======================================================================\n",
    "# main\n",
    "#======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data=pd.read_csv('data_200k.csv')\n",
    "print('Imported data:')\n",
    "print(data.head(4))\n",
    "\n",
    "# put columns into numpy nd-arrays \n",
    "# myy is the fit variable\n",
    "myy = data['myy'].values\n",
    "# signal/background label is 1/0\n",
    "y = data['label'].values\n",
    "# classifier inputs are photon 4-momenta\n",
    "X = data[['pt_y1','eta_y1','phi_y1','e_y1',\n",
    "         'pt_y2','eta_y2','phi_y2','e_y2']].values\n",
    "      \n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, myy_train, myy_test \\\n",
    "    = train_test_split(X, y, myy, test_size = 0.30, random_state = 42)\n",
    "\n",
    "# Normalize X data\n",
    "normalize = StandardScaler()\n",
    "X_train = normalize.fit_transform(X_train)\n",
    "X_test = normalize.fit_transform(X_test)\n",
    "\n",
    "# Number of samples, features, epochs & batch size\n",
    "num_samples = X_train.shape[0]\n",
    "num_feat = X_train.shape[1]\n",
    "num_epochs = 10\n",
    "batch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n =============== 1) Standalone classifier  =============== \\n ')\n",
    "clf_standalone = classifier(num_feat)\n",
    "clf_standalone.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "hist_clf_standalone = clf_standalone.fit(X_train, y_train,\n",
    "                                         epochs = num_epochs,\n",
    "                                         batch_size = batch,\n",
    "                                         validation_split = 0.2,\n",
    "                                         verbose = 2)\n",
    "\n",
    "# generate predictions & store results \n",
    "y_pred = clf_standalone.predict(X_test).flatten()\n",
    "res = np.array([myy_test.T, y_pred, y_test])\n",
    "np.savetxt('clf_standalone_results.csv', res.T, delimiter = ',')\n",
    "\n",
    "# store training history \n",
    "with open('clf_standalone_history.pckl', 'wb') as file_pi:\n",
    "    pickle.dump(hist_clf_standalone.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n ================== 2) Adversarial training ================ \\n ')\n",
    "\n",
    "# Define parameters for combined network\n",
    "lambda_reg = 3             # Regularization parameter \n",
    "num_gmm = 5                # Number of Gaussian Mixture Model components\n",
    "lr = 1e-5                 # Relative learning rates for classifier and adversary\n",
    "\n",
    "loss_weights = [lr, lambda_reg]\n",
    "\n",
    "# Prepare sample weights (i.e. only do mass-decorrelation for background)\n",
    "sample_weight = [np.ones(num_samples, dtype=float), (y_train == 0).astype(float)]\n",
    "sample_weight[1] *= np.sum(sample_weight[0])/ np.sum(sample_weight[1])   \n",
    "\n",
    "# Rescale diphoton invariant mass to [0,1]\n",
    "sc_myy_train = myy_train - myy_train.min()\n",
    "sc_myy_train /= myy_train.max()\n",
    "\n",
    "'''\n",
    "Define classifier, adversary & combined network\n",
    "'''\n",
    "\n",
    "clf = classifier(num_feat)\n",
    "adv = adversary(num_gmm)\n",
    "ANN = combined(clf, adv, lambda_reg, lr)\n",
    "\n",
    "# Build & train combined model\n",
    "ANN.compile(optimizer='adam', loss=['binary_crossentropy', custom_loss], loss_weights = loss_weights)\n",
    "hist_ANN = ANN.fit([X_train, sc_myy_train], [y_train, np.ones_like(sc_myy_train)], \n",
    "                   sample_weight = sample_weight, epochs = num_epochs, batch_size = batch, \n",
    "                   validation_split = 0.2, verbose = 2)\n",
    "\n",
    "\n",
    "'''\n",
    "Generate output files\n",
    "'''\n",
    "\n",
    "# Test set predictions\n",
    "y_pred = clf.predict(X_test).flatten()\n",
    "\n",
    "# Write myy, predictions and label to file\n",
    "res = np.array([myy_test.T, y_pred, y_test])\n",
    "np.savetxt('ANN_results.csv', res.T, delimiter = ',')\n",
    "\n",
    "# store training history \n",
    "with open('ANN_history.pckl', 'wb') as file_pi:\n",
    "    pickle.dump(hist_ANN.history, file_pi)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
